{
    "hyperparameters": {
        "learning-rate": [
            0.0035,
            [
                0.002,
                12
            ],
            [
                0.0009,
                28
            ],
            [
                0.0003,
                31
            ]
        ],
        "optimizer": "Adam",
        "loss-function": "NLLLoss",
        "init-learning-rate": 0.0035,
        "batch-size": 1000,
        "cnn-layers": 1,
        "cnn-channels": [
            64
        ],
        "kernel-size": 3,
        "hidden-size": 200,
        "rnn-layers": 2,
        "decay-rate": 0.985,
        "decay-intervall": 105,
        "dropout-chance": 0.3,
        "embedding-size": 200,
        "augmentation": 0.5
    },
    "results": {
        "train-accuracy": [
            21.36791,
            71.237,
            79.14625,
            81.79731,
            82.83162,
            83.64775,
            84.03257,
            84.51605,
            84.91036,
            85.1902,
            85.46308,
            85.83937,
            89.60253,
            86.52364,
            85.04727,
            85.4166,
            85.5317,
            85.71636,
            85.86814,
            86.07115,
            86.23431,
            86.40285,
            86.61375,
            86.75826,
            86.87779,
            87.06308,
            87.23668,
            87.36854,
            91.70055,
            87.25281,
            87.02198,
            92.15146
        ],
        "train-loss": [
            2.3032544757289464,
            0.9999115416304171,
            0.771678619580314,
            0.6748689063339955,
            0.6369526861969981,
            0.6086446739709716,
            0.5899286298345692,
            0.5727657425666833,
            0.557346948125761,
            0.5465057216796213,
            0.5371116871713464,
            0.5261178698261454,
            0.4026757262482628,
            0.4868807091321855,
            0.5263954008979376,
            0.5166223031301228,
            0.5109935166895954,
            0.5048029143539513,
            0.49717708500777885,
            0.49035376611192144,
            0.4841131242667839,
            0.4778408855669882,
            0.4700650229627026,
            0.46544804971676895,
            0.4614747486279966,
            0.4550131417411359,
            0.448734785762119,
            0.4430671838179748,
            0.30835409941921477,
            0.4356831403372792,
            0.43990215640338814,
            0.2835488590156242
        ],
        "validation-accuracy": [
            48.06488,
            78.40068,
            81.62777,
            82.83438,
            82.90837,
            83.85885,
            84.44508,
            84.74104,
            84.7638,
            85.26466,
            85.5037,
            85.25896,
            85.42971,
            85.60615,
            85.64599,
            85.58907,
            85.45817,
            85.86227,
            85.7712,
            85.54354,
            85.94195,
            85.56061,
            85.93056,
            85.71998,
            86.06147,
            85.83381,
            85.98748,
            85.85088,
            86.07285,
            86.0387,
            85.87934,
            86.43142
        ],
        "validation-loss": [
            1.5585833655463324,
            0.7805312838819292,
            0.6766534911261665,
            0.6328922675715553,
            0.6185424195395576,
            0.5996773607201047,
            0.5755061639679803,
            0.5731516314877404,
            0.5684162643220689,
            0.552184897992346,
            0.5525158411926694,
            0.5487511290444268,
            0.5692005770073997,
            0.5430568638775084,
            0.5449861271513833,
            0.5433796147505442,
            0.5422797881894641,
            0.5376693805058798,
            0.5385407987568114,
            0.5415405250257916,
            0.5392490973075231,
            0.5481109718481699,
            0.544697547952334,
            0.5469390584362878,
            0.54641612039672,
            0.5517610377735562,
            0.5479091985358132,
            0.5478055278460184,
            0.5612596786684461,
            0.5409392433034049,
            0.5501748190985786,
            0.5596807665295072
        ]
    }
}